{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jesson-Wei/Bayesian-inference-based-GNN/blob/main/Neurips2022_Understanding_Non_linearity_in_Graph_Neural_Networks_from_the_Bayesian_Inference_Perspective_Experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Neurips 2022** \"Understanding Non-linearity in Graph Neural Networks from the Bayesian-Inference Perspective\" by Rongzhe Wei, Haoteng Yin, Junteng Jia, Austin R. Benson, Pan Li. **Link**: [Paper](https://arxiv.org/abs/2207.11311)"
      ],
      "metadata": {
        "id": "VbixVtoxmrE5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First, we recap the main conclusions of this paper:\n",
        "* When the node attributes are less informative compared to the structural information, non-linear propagation and linear propagation have almost the same mis-classification error.\n",
        "* When the node attributes are more informative, non-linear propagation shows\n",
        "advantages. The mis-classification error of non-linear propagation can be significantly smaller than that of linear propagation with sufficiently informative node attributes.\n",
        "* When there is a distribution shift of the node attributes between the training and testing datasets, non-linearity provides better transferability in the regime of informative node attributes."
      ],
      "metadata": {
        "id": "boXaQpUAqnxU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Second, we demonstrate the real-data experiments under PubMed, Cora, CiteSeer datasets."
      ],
      "metadata": {
        "id": "SY_0vjp3se6q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up experiments"
      ],
      "metadata": {
        "id": "91IDsB0ltzWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add this in a Google Colab cell to install the correct version of Pytorch Geometric.\n",
        "import torch\n",
        "\n",
        "def format_pytorch_version(version):\n",
        "  return version.split('+')[0]\n",
        "\n",
        "TORCH_version = torch.__version__\n",
        "TORCH = format_pytorch_version(TORCH_version)\n",
        "print(f\"PyTorch version: {TORCH}\")\n",
        "\n",
        "def format_cuda_version(version):\n",
        "  return 'cu' + version.replace('.', '')\n",
        "\n",
        "CUDA_version = torch.version.cuda\n",
        "CUDA = format_cuda_version(CUDA_version)\n",
        "print(f\"CUDA Version: {CUDA}\")\n",
        "\n",
        "!pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-geometric \n",
        "\n",
        "!pip install SciencePlots\n",
        "!pip install git+https://github.com/garrettj403/SciencePlots.git"
      ],
      "metadata": {
        "id": "WPEYVpy21Gsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import math\n",
        "\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.loader import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.datasets import Planetoid\n",
        "import torch_geometric.utils as utils\n",
        "import torch.nn as nn\n",
        "from torch import linalg as LA"
      ],
      "metadata": {
        "id": "IS6VsI0atcBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check whether \"science\" style is available\n",
        "'science' in plt.style.available"
      ],
      "metadata": {
        "id": "XcCvEfjEVcG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import datasets"
      ],
      "metadata": {
        "id": "FJH98Pls2O1_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paths = ['/tmp/PubMed', '/tmp/Cora', '/tmp/Citeseer']\n",
        "names = ['PubMed', 'Cora', 'Citeseer']\n",
        "###Select dataset: 0 for PubMed, 1 for Cora, 2 for Citeseer\n",
        "dataset = names[2]\n",
        "graph = Planetoid(root=f'/tmp/{dataset}', name=dataset)\n",
        "data = graph[0]\n",
        "print(data)\n",
        "print(f'#class: {graph.num_classes}, #feature_dim: {graph.num_node_features}')"
      ],
      "metadata": {
        "id": "JoLvcJgl0vjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check whether GPU is available"
      ],
      "metadata": {
        "id": "pe8e8mPX2fzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "id": "-Q58zHka2evY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate adjacency matrix of the graph"
      ],
      "metadata": {
        "id": "0tW1EdPQ2t9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Adj = utils.to_dense_adj(data.edge_index).squeeze(dim=0).to(device)\n",
        "Adj"
      ],
      "metadata": {
        "id": "jvlxvtgn2pD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the parameters of generated distributions and generating functions"
      ],
      "metadata": {
        "id": "-qMxXCIn3AUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if dataset in ['PubMed', 'Cora', 'Citeseer']:\n",
        "    #training epochs\n",
        "    epochs = 100\n",
        "    #dimension of node attributes\n",
        "    d = 10\n",
        "    #Gaussian mean\n",
        "    mu = 0.03 * np.ones(d)\n",
        "    # mus = np.array([i * np.ones(d) for i in np.array([0.05, 0.1, 0.2, 0.3, 0.4, 0.5])])\n",
        "    # mus = np.array([i * np.ones(d) for i in np.array([0.01, 0.025, 0.05, 0.075, 0.1, 0.125, 0.15, 0.175, 0.2])])\n",
        "    mus = np.array([i * np.ones(d) for i in np.array([0.1, 0.2, 0.3, 0.4, 0.5])])\n",
        "else:\n",
        "    raise NotImplementedError"
      ],
      "metadata": {
        "id": "8rDImbMh24EO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate two-class Gaussian node attributes\n",
        "def high_dim_Gaussian(Node_labels, d, mu):\n",
        "    I = np.identity(d) / d\n",
        "    X_feature = np.random.multivariate_normal(mu, I, len(Node_labels), 'raise')\n",
        "    X_feature = X_feature * Node_labels.reshape(-1, 1)\n",
        "    return X_feature\n",
        "\n",
        "#Generate two-class Laplacian node attributes\n",
        "def high_dim_Laplace(Node_labels, d, mu):\n",
        "    X_feature = np.random.laplace(loc=mu, scale=1.0, size=(len(Node_labels), d))\n",
        "    X_feature = X_feature * Node_labels.reshape(-1, 1)\n",
        "    return X_feature\n",
        "\n",
        "#Angle between two vectors are used to evaluate whether the classification plane is well aligned with the optimal plane\n",
        "def angle(vec1, vec2):\n",
        "    vec_angle = vec1 @ vec2 / (LA.norm(vec1) * LA.norm(vec2))\n",
        "    return vec_angle"
      ],
      "metadata": {
        "id": "YPBdfs7I4kzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Define"
      ],
      "metadata": {
        "id": "oT7VLiSH4QkN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the class for linear model\n",
        "class LinearAgg(torch.nn.Module):\n",
        "    def __init__(self, d, weighted=False):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(d, 1, bias=False)\n",
        "        self.weight = nn.Parameter(torch.tensor(1.0), requires_grad=True) if weighted else None\n",
        "\n",
        "    def forward(self, Adj, x):\n",
        "        x = x.float()\n",
        "        x = self.linear(x)\n",
        "        x_copy = x.clone()\n",
        "        if self.weight is not None:\n",
        "            x = x_copy + self.weight * (Adj @ x)\n",
        "        else:\n",
        "            x = x_copy + Adj @ x\n",
        "\n",
        "        return torch.sigmoid(x), self.linear.weight\n",
        "\n",
        "#Define the class for optimal non-linear model (Gaussian)\n",
        "class OptimalMP(torch.nn.Module):\n",
        "    def __init__(self, d):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(d, 1, bias=False)\n",
        "        self.thres = nn.Parameter(torch.tensor(0.2), requires_grad=True)\n",
        "\n",
        "    def forward(self, Adj, x):\n",
        "        x = x.float()\n",
        "        x = self.linear(x)\n",
        "        x_copy = x.clone()\n",
        "        x[x > self.thres] = self.thres\n",
        "        x[x < -self.thres] = -self.thres\n",
        "        x = x_copy + Adj @ x\n",
        "\n",
        "        return torch.sigmoid(x), self.linear.weight, self.thres\n",
        "\n",
        "#Define the class for optimal non-linear model (Laplacian)\n",
        "class OptimalMP_Laplacian(torch.nn.Module):\n",
        "    def __init__(self, d, weighted=False):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(d, 1, bias=False)\n",
        "        self.thres_1 = nn.Parameter(torch.tensor(2.0), requires_grad=True)\n",
        "        self.thres_2 = nn.Parameter(torch.tensor(2.0), requires_grad=True)\n",
        "\n",
        "    def forward(self, Adj, x):\n",
        "        x = x.float()\n",
        "        x[x > self.thres_1] = self.thres_1\n",
        "        x[x < -self.thres_1] = -self.thres_1\n",
        "        x = self.linear(x)\n",
        "        x_copy = x.clone()\n",
        "        x[x > self.thres_2] = self.thres_2\n",
        "        x[x < -self.thres_2] = -self.thres_2\n",
        "        x = x_copy + Adj @ x\n",
        "\n",
        "        return torch.sigmoid(x), self.linear.weight, self.thres_1, self.thres_2\n",
        "\n",
        "\n",
        "#Define the class for non-linear model with only feature transformation (Laplacian)\n",
        "class OptimalMP_Laplacian_Psi(torch.nn.Module):\n",
        "    def __init__(self, d):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(d, 1, bias=False)\n",
        "        self.thres_1 = nn.Parameter(torch.tensor(2.0), requires_grad=True)\n",
        "\n",
        "    def forward(self, Adj, x):\n",
        "        x = x.float()\n",
        "        x[x > self.thres_1] = self.thres_1\n",
        "        x[x < -self.thres_1] = -self.thres_1\n",
        "        x = self.linear(x)\n",
        "        x_copy = x.clone()\n",
        "        x = x_copy + Adj @ x\n",
        "\n",
        "        return torch.sigmoid(x), self.linear.weight, self.thres_1\n",
        "\n",
        "\n",
        "#Define the class for non-linear model with only non-linear propagation (Laplacian)\n",
        "class OptimalMP_Laplacian_Phi(torch.nn.Module):\n",
        "    def __init__(self, d):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(d, 1, bias=False)\n",
        "        self.thres_2 = nn.Parameter(torch.tensor(2.0), requires_grad=True)\n",
        "\n",
        "    def forward(self, Adj, x):\n",
        "        x = x.float()\n",
        "        x = self.linear(x)\n",
        "        x_copy = x.clone()\n",
        "        x[x > self.thres_2] = self.thres_2\n",
        "        x[x < -self.thres_2] = -self.thres_2\n",
        "        x = x_copy + Adj @ x\n",
        "\n",
        "        return torch.sigmoid(x), self.linear.weight, self.thres_2\n"
      ],
      "metadata": {
        "id": "KWpgOQeA4PxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Training"
      ],
      "metadata": {
        "id": "le1qQny33nI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define epochs for the experiments\n",
        "repeat = 1"
      ],
      "metadata": {
        "id": "pEGRSOFs3h3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gaussian distribution assumption\n"
      ],
      "metadata": {
        "id": "8mt1zvdr5CLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_gaussian(mus, d, repeat, data, Adj, device, epochs, num_classes, dataset):\n",
        "    for lidx in range(num_classes):\n",
        "        Node_labels = np.where((data.y == lidx), -1, 1)\n",
        "        label = torch.from_numpy(np.where((data.y == lidx), -1, 1)).unsqueeze(dim=1).to(device).float()\n",
        "        # Node_labels_loss = np.where((data.y == lidx), 0, 1)\n",
        "        label_loss = torch.from_numpy(np.where((data.y == lidx), 0, 1)).unsqueeze(dim=1).to(device).float()\n",
        "\n",
        "        results, weights = [], []\n",
        "        for mu in mus:\n",
        "            record = np.zeros([repeat, 6])\n",
        "            mu_torch = torch.from_numpy(mu).float()\n",
        "            for i in range(repeat):\n",
        "                print('=' * 20)\n",
        "                print(f'mu = {mu}')\n",
        "                X_feature = high_dim_Gaussian(Node_labels, d, mu)\n",
        "                X_feature_torch = torch.from_numpy(X_feature).to(device)\n",
        "                X_feature_test = high_dim_Gaussian(Node_labels, d, mu)\n",
        "                X_feature_torch_test = torch.from_numpy(X_feature_test).to(device)\n",
        "                # train original acc\n",
        "                acc_org = (np.sign(X_feature @ mu) == Node_labels).sum() / data.num_nodes\n",
        "                print(f'acc_org={acc_org}')\n",
        "                # test original acc\n",
        "                acc_org_eval = (np.sign(X_feature_test @ mu) == Node_labels).sum() / data.num_nodes\n",
        "\n",
        "                model_linear = LinearAgg(d).to(device)\n",
        "                optimizer1 = torch.optim.Adam(model_linear.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "                criterion1 = nn.BCELoss()\n",
        "\n",
        "                model_mp = OptimalMP(d).to(device)\n",
        "                optimizer2 = torch.optim.Adam(model_mp.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "                criterion2 = nn.BCELoss()\n",
        "\n",
        "                model_linear.train()\n",
        "                model_mp.train()\n",
        "                for epoch in range(epochs):\n",
        "                    optimizer1.zero_grad()\n",
        "                    optimizer2.zero_grad()\n",
        "                    out_linear, out_linear_weight = model_linear(Adj, X_feature_torch)\n",
        "                    out_mp, out_mp_weight, out_thres = model_mp(Adj, X_feature_torch)\n",
        "                    predicted_label_linear = torch.sign(out_linear - 0.5)\n",
        "                    predicted_label_mp = torch.sign(out_mp - 0.5)\n",
        "                    loss_linear = criterion1(out_linear, label_loss)\n",
        "                    loss_mp = criterion2(out_mp, label_loss)\n",
        "                    acc_linear = (predicted_label_linear == label).sum() / data.num_nodes\n",
        "                    acc_mp = (predicted_label_mp == label).sum() / data.num_nodes\n",
        "                    loss_linear.backward()\n",
        "                    loss_mp.backward()\n",
        "                    optimizer1.step()\n",
        "                    optimizer2.step()\n",
        "                    if epoch % 50 == 0:\n",
        "                        print(f'Epoch:{epoch}')\n",
        "                        print(\n",
        "                            f'Linear weight angle:{angle(out_linear_weight.detach().cpu().squeeze(), mu_torch).item()}')\n",
        "                        print(f'MP weight angle:{angle(out_mp_weight.detach().cpu().squeeze(), mu_torch).item()}')\n",
        "                        print(f'MP Threshold:{out_thres}')\n",
        "                        print(f'Linear training loss:{loss_linear}')\n",
        "                        print(f'MP training loss:{loss_mp}')\n",
        "                        print(f'acc_linear:{acc_linear}')\n",
        "                        print(f'acc_mp:{acc_mp}')\n",
        "\n",
        "                model_linear.eval()\n",
        "                model_mp.eval()\n",
        "\n",
        "                out_linear_eval, out_linear_weight_eval = model_linear(Adj, X_feature_torch_test)\n",
        "                out_mp_eval, out_mp_weight_eval, out_thres_eval = model_mp(Adj, X_feature_torch_test)\n",
        "\n",
        "                predicted_label_linear_eval = torch.sign(out_linear_eval - 0.5)\n",
        "                predicted_label_mp_eval = torch.sign(out_mp_eval - 0.5)\n",
        "\n",
        "                acc_linear_eval = (predicted_label_linear_eval == label).sum() / data.num_nodes\n",
        "                acc_mp_eval = (predicted_label_mp_eval == label).sum() / data.num_nodes\n",
        "                print('*' * 20)\n",
        "                print(f'acc_linear:{acc_linear_eval}')\n",
        "                print(f'acc_mp:{acc_mp_eval}')\n",
        "                record[i] = [acc_org, acc_org_eval, acc_mp.item(), acc_mp_eval.item(), acc_linear.item(),\n",
        "                             acc_linear_eval.item()]\n",
        "            original_avg, original_avg_eval, mp_avg, mp_avg_eval, linear_avg, linear_avg_eval = record.mean(axis=0)\n",
        "            original_std, original_std_eval, mp_std, mp_std_eval, linear_std, linear_std_eval = record.std(axis=0)\n",
        "            results.append([LA.norm(mu_torch).item(),\n",
        "                            original_avg, mp_avg, linear_avg,\n",
        "                            original_avg_eval, mp_avg_eval, linear_avg_eval,\n",
        "                            original_std, original_std_eval, mp_std, mp_std_eval, linear_std, linear_std_eval])\n",
        "            # weights.append(record[:, -1])\n",
        "        # np.save(f'./results/semi_synthetic_Gau_{dataset}_l{lidx}.npy', results)\n",
        "        total_results.append(results)\n",
        "    return total_results"
      ],
      "metadata": {
        "id": "7JfuU5gM5Iwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Laplacian distribution assumption"
      ],
      "metadata": {
        "id": "1aaIsvJO5Z80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_laplacian(mus, d, repeat, data, Adj, device, epochs, num_classes, dataset):\n",
        "    total_results = []\n",
        "    for lidx in range(num_classes):\n",
        "        Node_labels = np.where((data.y == lidx), -1, 1)\n",
        "        label = torch.from_numpy(np.where((data.y == lidx), -1, 1)).unsqueeze(dim=1).to(device).float()\n",
        "        # Node_labels_loss = np.where((data.y == lidx), 0, 1)\n",
        "        label_loss = torch.from_numpy(np.where((data.y == lidx), 0, 1)).unsqueeze(dim=1).to(device).float()\n",
        "\n",
        "        results, weights = [], []\n",
        "        for mu in mus:\n",
        "            record = np.zeros([repeat, 10])\n",
        "            mu_torch = torch.from_numpy(mu).float()\n",
        "            for i in range(repeat):\n",
        "                print('=' * 20)\n",
        "                print(f'mu = {mu}')\n",
        "                X_feature = high_dim_Laplace(Node_labels, d, mu)\n",
        "                X_feature_torch = torch.from_numpy(X_feature).to(device)\n",
        "                X_feature_test = high_dim_Laplace(Node_labels, d, mu)\n",
        "                X_feature_torch_test = torch.from_numpy(X_feature_test).to(device)\n",
        "                # train original acc\n",
        "                acc_org = (np.sign(X_feature @ mu) == Node_labels).sum() / data.num_nodes\n",
        "                print(f'acc_org={acc_org}')\n",
        "                # test original acc\n",
        "                acc_org_eval = (np.sign(X_feature_test @ mu) == Node_labels).sum() / data.num_nodes\n",
        "\n",
        "                model_linear = LinearAgg(d).to(device)\n",
        "                optimizer1 = torch.optim.Adam(model_linear.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "                criterion1 = nn.BCELoss()\n",
        "\n",
        "                model_mp = OptimalMP_Laplacian(d).to(device)\n",
        "                optimizer2 = torch.optim.Adam(model_mp.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "                criterion2 = nn.BCELoss()\n",
        "\n",
        "                model_phi = OptimalMP_Laplacian_Phi(d).to(device)\n",
        "                optimizer3 = torch.optim.Adam(model_phi.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "                criterion3 = nn.BCELoss()\n",
        "\n",
        "                model_psi = OptimalMP_Laplacian_Psi(d).to(device)\n",
        "                optimizer4 = torch.optim.Adam(model_psi.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "                criterion4 = nn.BCELoss()\n",
        "\n",
        "                model_linear.train()\n",
        "                model_mp.train()\n",
        "                for epoch in range(epochs):\n",
        "                    optimizer1.zero_grad()\n",
        "                    optimizer2.zero_grad()\n",
        "                    optimizer3.zero_grad()\n",
        "                    optimizer4.zero_grad()\n",
        "                    out_linear, out_linear_weight = model_linear(Adj, X_feature_torch)\n",
        "                    out_mp, out_mp_weight, out_thres_1, out_thres_2 = model_mp(Adj, X_feature_torch)\n",
        "                    out_phi, out_phi_weight, out_thres_phi = model_phi(Adj, X_feature_torch)\n",
        "                    out_psi, out_psi_weight, out_thres_psi = model_psi(Adj, X_feature_torch)\n",
        "                    predicted_label_linear = torch.sign(out_linear - 0.5)\n",
        "                    predicted_label_mp = torch.sign(out_mp - 0.5)\n",
        "                    predicted_label_phi = torch.sign(out_phi - 0.5)\n",
        "                    predicted_label_psi = torch.sign(out_psi - 0.5)\n",
        "                    loss_linear = criterion1(out_linear, label_loss)\n",
        "                    loss_mp = criterion2(out_mp, label_loss)\n",
        "                    loss_phi = criterion3(out_phi, label_loss)\n",
        "                    loss_psi = criterion4(out_psi, label_loss)\n",
        "                    acc_linear = (predicted_label_linear == label).sum() / data.num_nodes\n",
        "                    acc_mp = (predicted_label_mp == label).sum() / data.num_nodes\n",
        "                    acc_phi = (predicted_label_phi == label).sum() / data.num_nodes\n",
        "                    acc_psi = (predicted_label_psi == label).sum() / data.num_nodes\n",
        "                    loss_linear.backward()\n",
        "                    loss_mp.backward()\n",
        "                    loss_phi.backward()\n",
        "                    loss_psi.backward()\n",
        "                    optimizer1.step()\n",
        "                    optimizer2.step()\n",
        "                    optimizer3.step()\n",
        "                    optimizer4.step()\n",
        "                    if epoch % 100 == 0:\n",
        "                        print(f'Epoch:{epoch}')\n",
        "                        print(\n",
        "                            f'Linear weight angle:{angle(out_linear_weight.detach().cpu().squeeze(), mu_torch).item()}')\n",
        "                        # print(f'MP Factor:{model_mp.weight.item()}')\n",
        "                        print(f'MP weight angle:{angle(out_mp_weight.detach().cpu().squeeze(), mu_torch).item()}')\n",
        "                        print(f'Phi weight angle:{angle(out_phi_weight.detach().cpu().squeeze(), mu_torch).item()}')\n",
        "                        print(f'Psi weight angle:{angle(out_psi_weight.detach().cpu().squeeze(), mu_torch).item()}')\n",
        "                        print(f'MP Threshold 1:{out_thres_1}')\n",
        "                        print(f'MP Threshold 2:{out_thres_2}')\n",
        "                        print(f'Threshold Phi:{out_thres_phi}')\n",
        "                        print(f'Threshold Psi:{out_thres_psi}')\n",
        "                        print(f'Linear training loss:{loss_linear}')\n",
        "                        print(f'MP training loss:{loss_mp}')\n",
        "                        print(f'Phi training loss:{loss_phi}')\n",
        "                        print(f'Psi training loss:{loss_psi}')\n",
        "                        print(f'acc_linear:{acc_linear}')\n",
        "                        print(f'acc_mp:{acc_mp}')\n",
        "                        print(f'acc_phi:{acc_phi}')\n",
        "                        print(f'acc_psi:{acc_psi}')\n",
        "\n",
        "                model_linear.eval()\n",
        "                model_mp.eval()\n",
        "                model_phi.eval()\n",
        "                model_psi.eval()\n",
        "\n",
        "                out_linear_eval, out_linear_weight_eval = model_linear(Adj, X_feature_torch_test)\n",
        "                out_mp_eval, out_mp_weight_eval, out_thres_eval_1, out_thres_eval_2 = model_mp(Adj,\n",
        "                                                                                               X_feature_torch_test)\n",
        "                out_phi_eval, out_phi_weight_eval, out_thres_eval_phi = model_phi(Adj, X_feature_torch_test)\n",
        "                out_psi_eval, out_psi_weight_eval, out_thres_eval_psi = model_psi(Adj, X_feature_torch_test)\n",
        "\n",
        "                predicted_label_linear_eval = torch.sign(out_linear_eval - 0.5)\n",
        "                predicted_label_mp_eval = torch.sign(out_mp_eval - 0.5)\n",
        "                predicted_label_phi_eval = torch.sign(out_phi_eval - 0.5)\n",
        "                predicted_label_psi_eval = torch.sign(out_psi_eval - 0.5)\n",
        "\n",
        "                acc_linear_eval = (predicted_label_linear_eval == label).sum() / data.num_nodes\n",
        "                acc_mp_eval = (predicted_label_mp_eval == label).sum() / data.num_nodes\n",
        "                acc_phi_eval = (predicted_label_phi_eval == label).sum() / data.num_nodes\n",
        "                acc_psi_eval = (predicted_label_psi_eval == label).sum() / data.num_nodes\n",
        "                print('*' * 20)\n",
        "                print(f'acc_linear:{acc_linear_eval}')\n",
        "                print(f'acc_mp:{acc_mp_eval}')\n",
        "                print(f'acc_phi:{acc_phi_eval}')\n",
        "                print(f'acc_psi:{acc_psi_eval}')\n",
        "\n",
        "                record[i] = [acc_org, acc_org_eval, acc_mp.item(), acc_mp_eval.item(), acc_linear.item(),\n",
        "                             acc_linear_eval.item(), acc_phi.item(), acc_phi_eval.item(), acc_psi.item(),\n",
        "                             acc_psi_eval.item()]\n",
        "            original_avg, original_avg_eval, mp_avg, mp_avg_eval, linear_avg, linear_avg_eval, phi_avg, phi_avg_eval, psi_avg, psi_avg_eval = record.mean(\n",
        "                axis=0)\n",
        "            original_std, original_std_eval, mp_std, mp_std_eval, linear_std, linear_std_eval, phi_std, phi_std_eval, psi_std, psi_std_eval = record.std(\n",
        "                axis=0)\n",
        "            results.append(\n",
        "                [LA.norm(torch.from_numpy(mu).float()).numpy(), original_avg, mp_avg, linear_avg, phi_avg, psi_avg,\n",
        "                 original_avg_eval,\n",
        "                 mp_avg_eval, linear_avg_eval, phi_avg_eval, psi_avg_eval, original_std, original_std_eval, mp_std,\n",
        "                 mp_std_eval, linear_std,\n",
        "                 linear_std_eval, phi_std, phi_std_eval, psi_std, psi_std_eval])\n",
        "            # weights.append(record[:, -1])\n",
        "        # np.save(f'./results/semi_synthetic_LP_{dataset}_l{lidx}.npy', results)\n",
        "        total_results.append(results)\n",
        "    return total_results"
      ],
      "metadata": {
        "id": "qwO7cbit5ZSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train either Gaussian or Laplacian\n",
        "# total_results = train_gaussian(mus, d, repeat, data, Adj, device, epochs, graph.num_classes, dataset)\n",
        "total_results = train_laplacian(mus, d, repeat, data, Adj, device, epochs, graph.num_classes, dataset)"
      ],
      "metadata": {
        "id": "fzct3YyS5rRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization"
      ],
      "metadata": {
        "id": "VBPBMuIj5_B7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#view one example\n",
        "pd.DataFrame(total_results[0], columns = ['mu_norm', 'original_avg', 'mp_avg', 'linear_avg', 'phi_avg', 'psi_avg', 'original_avg_eval', 'mp_avg_eval', 'linear_avg_eval', 'phi_avg_eval', 'psi_avg_eval', 'original_std', 'original_std_eval', 'mp_std', 'mp_std_eval', 'linear_std', 'linear_std_eval', 'phi_std', 'phi_std_eval', 'psi_std', 'psi_std_eval'])"
      ],
      "metadata": {
        "id": "EaKRlVoL6B9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate Average Performance"
      ],
      "metadata": {
        "id": "YvrsL9n1FqAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #Avg Pubmed\n",
        "# df_pubmed_OVA = pd.DataFrame(np.load('./results_New/semi_synthetic_LP_PubMed_l0.npy'), columns = ['mu_norm', 'original_avg', 'mp_avg', 'linear_avg', 'phi_avg', 'psi_avg', 'original_avg_eval', 'mp_avg_eval', 'linear_avg_eval', 'phi_avg_eval', 'psi_avg_eval', 'original_std', 'original_std_eval', 'mp_std', 'mp_std_eval', 'linear_std', 'linear_std_eval', 'phi_std', 'phi_std_eval', 'psi_std', 'psi_std_eval'])\n",
        "# for i in range(1, 3):\n",
        "#     df_pubmed_OVA = df_pubmed_OVA + pd.DataFrame(np.load(f'./results_New/semi_synthetic_LP_PubMed_l{i}.npy'), columns = ['mu_norm', 'original_avg', 'mp_avg', 'linear_avg', 'phi_avg', 'psi_avg', 'original_avg_eval', 'mp_avg_eval', 'linear_avg_eval', 'phi_avg_eval', 'psi_avg_eval', 'original_std', 'original_std_eval', 'mp_std', 'mp_std_eval', 'linear_std', 'linear_std_eval', 'phi_std', 'phi_std_eval', 'psi_std', 'psi_std_eval'])\n",
        "# df_pubmed_OVA = df_pubmed_OVA / 3\n",
        "# #Avg Cora\n",
        "# df_cora_OVA = pd.DataFrame(np.load('./results_New/semi_synthetic_LP_Cora_l0.npy'), columns = ['mu_norm', 'original_avg', 'mp_avg', 'linear_avg', 'phi_avg', 'psi_avg', 'original_avg_eval', 'mp_avg_eval', 'linear_avg_eval', 'phi_avg_eval', 'psi_avg_eval', 'original_std', 'original_std_eval', 'mp_std', 'mp_std_eval', 'linear_std', 'linear_std_eval', 'phi_std', 'phi_std_eval', 'psi_std', 'psi_std_eval'])\n",
        "# for i in range(1, 7):\n",
        "#     df_cora_OVA = df_cora_OVA + pd.DataFrame(np.load(f'./results_New/semi_synthetic_LP_Cora_l{i}.npy'), columns = ['mu_norm', 'original_avg', 'mp_avg', 'linear_avg', 'phi_avg', 'psi_avg', 'original_avg_eval', 'mp_avg_eval', 'linear_avg_eval', 'phi_avg_eval', 'psi_avg_eval', 'original_std', 'original_std_eval', 'mp_std', 'mp_std_eval', 'linear_std', 'linear_std_eval', 'phi_std', 'phi_std_eval', 'psi_std', 'psi_std_eval'])\n",
        "# df_cora_OVA = df_cora_OVA / 7\n",
        "#Avg Citeseer\n",
        "df_citeseer_OVA = pd.DataFrame(total_results[0], columns = ['mu_norm', 'original_avg', 'mp_avg', 'linear_avg', 'phi_avg', 'psi_avg', 'original_avg_eval', 'mp_avg_eval', 'linear_avg_eval', 'phi_avg_eval', 'psi_avg_eval', 'original_std', 'original_std_eval', 'mp_std', 'mp_std_eval', 'linear_std', 'linear_std_eval', 'phi_std', 'phi_std_eval', 'psi_std', 'psi_std_eval'])\n",
        "for i in range(1, 6):\n",
        "    df_citeseer_OVA = df_citeseer_OVA + pd.DataFrame(total_results[i], columns = ['mu_norm', 'original_avg', 'mp_avg', 'linear_avg', 'phi_avg', 'psi_avg', 'original_avg_eval', 'mp_avg_eval', 'linear_avg_eval', 'phi_avg_eval', 'psi_avg_eval', 'original_std', 'original_std_eval', 'mp_std', 'mp_std_eval', 'linear_std', 'linear_std_eval', 'phi_std', 'phi_std_eval', 'psi_std', 'psi_std_eval'])\n",
        "df_citeseer_OVA = df_citeseer_OVA / 6"
      ],
      "metadata": {
        "id": "q-moBBgt65uV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc_org_citeseer_eval = df_citeseer_OVA['original_avg_eval']\n",
        "acc_mp_citeseer_eval = df_citeseer_OVA['mp_avg_eval']\n",
        "acc_linear_citeseer_eval = df_citeseer_OVA['linear_avg_eval']\n",
        "acc_org_citeseer_std_eval = df_citeseer_OVA['original_std_eval']\n",
        "acc_mp_citeseer_std_eval = df_citeseer_OVA['mp_std_eval']\n",
        "acc_linear_citeseer_std_eval = df_citeseer_OVA['linear_std_eval']\n",
        "acc_mp_phi_citeseer_eval = df_citeseer_OVA['phi_avg_eval']\n",
        "acc_mp_psi_citeseer_eval = df_citeseer_OVA['psi_avg_eval']\n",
        "acc_mp_phi_citeseer_std_eval = df_citeseer_OVA['phi_std_eval']\n",
        "acc_mp_psi_citeseer_std_eval = df_citeseer_OVA['psi_std_eval']"
      ],
      "metadata": {
        "id": "buqfWbPuToYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print 1/2*||\\mu - \\nu||_2\n",
        "mu_norm = df_citeseer_OVA['mu_norm']\n",
        "mu_norm"
      ],
      "metadata": {
        "id": "wzfa78DJT8_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with plt.style.context(['science','no-latex']):\n",
        "    fig, ax = plt.subplots(figsize=(9, 6))\n",
        "    ax.errorbar(2*mu_norm, acc_mp_citeseer_eval, acc_mp_citeseer_std_eval, linestyle = '-', linewidth = 3, marker = 'o', label='Optimal Non-linear Propagation')\n",
        "    ax.errorbar(2*mu_norm, acc_linear_citeseer_eval, acc_linear_citeseer_std_eval, linestyle = 'dashdot', linewidth = 3, marker = 'v', label='Linear Model')\n",
        "    ax.errorbar(2*mu_norm, acc_mp_phi_citeseer_eval, acc_mp_phi_citeseer_std_eval, linestyle = 'dashdot', linewidth = 3, marker = '1', label='Only Phi Non-linear Propagation')\n",
        "    ax.errorbar(2*mu_norm, acc_mp_psi_citeseer_eval, acc_mp_psi_citeseer_std_eval, linestyle = 'dashdot', linewidth = 3, alpha = 0.6, marker = '2', label='Only Psi Attribute Tansformation')\n",
        "    ax.errorbar(2*mu_norm, acc_org_citeseer_eval, acc_org_citeseer_std_eval, linestyle = 'dotted', linewidth = 3, marker = 's', label='No Propagation')\n",
        "\n",
        "    #     ax.set_title('Classification Accuracy on PubMed Dataset (Training Phase)', fontsize = 15)\n",
        "    ax.set_title('CiteSeer - Laplacian', fontsize = 24)\n",
        "    ax.legend(loc = 'lower right', fontsize = 18)\n",
        "    ax.autoscale(tight=True)\n",
        "    plt.ylim([0.4, 1])\n",
        "    ax.tick_params(axis='both',\n",
        "             labelsize=18, \n",
        "             color='black',    \n",
        "             labelcolor='black', \n",
        "             direction='in'\n",
        "              ) \n",
        "    plt.xlabel('Attributed Information ($||\\mu - \\nu||_2$)', fontsize = 24)\n",
        "    plt.ylabel('Accuracy', fontsize = 24)\n",
        "    # plt.savefig('OVA_CiteSeer_Testing_Laplacian_sufficient.pdf')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "i-VyDV1yUNi_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}